{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c4e1c2-931b-4e68-9378-5d376b0df0ef",
   "metadata": {
    "id": "17c4e1c2-931b-4e68-9378-5d376b0df0ef"
   },
   "source": [
    "# Programming Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d597afc-35f8-4b03-8084-de771032604c",
   "metadata": {
    "id": "7d597afc-35f8-4b03-8084-de771032604c"
   },
   "source": [
    "**Name:** <br />\n",
    "**Roll No:**\n",
    "***\n",
    "\n",
    "## Instructions\n",
    "\n",
    "\n",
    "- Kindly name your submission files as `RollNo_Name_PA2.ipynb`.  <br />\n",
    "- You are required to work out your answers and submit only the iPython Notebook. The code should be well commented and easy to understand as there are marks for this. This notebook can be used as a template for assignment submission. <br />\n",
    "- Submissions are to be made through iPearl portal. Submissions made through mail will not be graded.<br />\n",
    "- Answers to the theory questions if any should be included in the notebook itself. While using special symbols use the $\\LaTeX$ mode <br />\n",
    "- Make sure your plots are clear and have title, legends and clear lines, etc. <br />\n",
    "- Plagiarism of any form will not be tolerated. If your solutions are found to match with other students or from other uncited sources, there will be heavy penalties and the incident will be reported to the disciplinary authorities. <br />\n",
    "- In case you have any doubts, feel free to reach out to TAs for help. <br />\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69751002-4656-47cf-8b2c-6016a434f4b6",
   "metadata": {
    "id": "69751002-4656-47cf-8b2c-6016a434f4b6"
   },
   "source": [
    "## E1: A Deterministic Career Path\n",
    "\n",
    "Consider a simple Markov Decision Process below with four states and two actions available at each state. In this simplistic setting actions have deterministic effects, i.e., taking an action in a state always leads to one next state with transition probability equal to one. There are two actions out of each state for the agent to choose from: D for development and R for research. The _ultimately-care-only-about-money_ reward scheme is given along with the states.\n",
    "\n",
    "<img src='assets/mdp-d.png' width=\"700\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0f991f3-9630-4656-9caa-135f13847ed8",
   "metadata": {
    "id": "b0f991f3-9630-4656-9caa-135f13847ed8"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47afefbb-7b00-44e5-82dd-16953b59a7f3",
   "metadata": {
    "id": "47afefbb-7b00-44e5-82dd-16953b59a7f3"
   },
   "source": [
    "### E1.1 Environment Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c01aa4-94c3-48e5-ad3b-279e03685260",
   "metadata": {
    "id": "61c01aa4-94c3-48e5-ad3b-279e03685260"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mRepresents a Career Path problem Gym Environment which provides a Fully observable\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mMDP\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCareerPathEnv\u001b[39;00m(gym\u001b[38;5;241m.\u001b[39mEnv):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    CareerPathEnv represents the Gym Environment for the Career Path problem environment\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    States : [0:'Unemployed',1:'Industry',2:'Grad School',3:'Academia']\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    Actions : [0:'Research', 1:'Development']\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender.modes\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Represents a Career Path problem Gym Environment which provides a Fully observable\n",
    "MDP\n",
    "'''\n",
    "class CareerPathEnv(gym.Env):\n",
    "    '''\n",
    "    CareerPathEnv represents the Gym Environment for the Career Path problem environment\n",
    "    States : [0:'Unemployed',1:'Industry',2:'Grad School',3:'Academia']\n",
    "    Actions : [0:'Research', 1:'Development']\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self,initial_state=0,no_states=4,no_actions=2):\n",
    "        '''\n",
    "        Constructor for the CareerPath class\n",
    "\n",
    "        Args:\n",
    "            initial_state : starting state of the agent\n",
    "            no_states : The no. of possible states which is 4\n",
    "            no_actions : The no. of possible actions which is 2\n",
    "\n",
    "        '''\n",
    "        self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "        self.nA = no_actions\n",
    "        self.nS = no_states\n",
    "        self.prob_dynamics = {\n",
    "            # s: {\n",
    "            #   a: [(p(s,s'|a), s', r', terminal/not)]\n",
    "            # }\n",
    "\n",
    "            0: {\n",
    "                0: [(1.0, 2, 0.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(1.0, 0, -10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Resets the environment\n",
    "        Returns:\n",
    "            observations containing player's current state\n",
    "        '''\n",
    "        self.state = self.initial_state\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        '''\n",
    "        Returns the player's state as the observation of the environment\n",
    "        '''\n",
    "        return (self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        '''\n",
    "        Renders the environment\n",
    "        '''\n",
    "        print(\"Current state: {}\".format(self.state))\n",
    "\n",
    "    def sample_action(self):\n",
    "        '''\n",
    "        Samples and returns a random action from the action space\n",
    "        '''\n",
    "        return random.randint(0, self.nA)\n",
    "    def P(self):\n",
    "        '''\n",
    "        Defines and returns the probabilty transition matrix which is in the form of a nested dictionary\n",
    "        '''\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(1.0, 2, 0.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(1.0, 0, -10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "        }\n",
    "        return self.prob_dynamics\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Performs the given action\n",
    "        Args:\n",
    "            action : action from the action_space to be taking in the environment\n",
    "        Returns:\n",
    "            observation - returns current state\n",
    "            reward - reward obtained after taking the given action\n",
    "            done - True if the episode is complete else False\n",
    "        '''\n",
    "        if action >= self.nA:\n",
    "            action = self.nA-1\n",
    "\n",
    "        dynamics_tuple = self.prob_dynamics[self.state][action][0]\n",
    "        self.state = dynamics_tuple[1]\n",
    "\n",
    "\n",
    "        return self.state, dynamics_tuple[2], dynamics_tuple[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9125c6e-8599-4dea-b388-b20596e33201",
   "metadata": {
    "id": "c9125c6e-8599-4dea-b388-b20596e33201"
   },
   "source": [
    "### E1.2 Policies\n",
    "\n",
    "After implementing the environment let us see how to make decisions in the environment. Let $\\pi_1(s) = R$ and $\\pi_2(s) = D$ for any state be two policies. Let us see how these policies look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d58aa48-25aa-4cb3-a70d-c4ac68f0cacc",
   "metadata": {
    "id": "7d58aa48-25aa-4cb3-a70d-c4ac68f0cacc",
    "outputId": "8d2c9d13-71f0-4ed9-b0d8-28b4eac47e1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research policy: \n",
      " [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "Development policy: \n",
      " [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Random policy: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]]\n",
      "Uncertain policy: \n",
      " [[0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "policy_R = np.concatenate((np.ones([4, 1]), np.zeros([4, 1])), axis=1)\n",
    "policy_D = np.concatenate((np.zeros([4, 1]), np.ones([4, 1])), axis=1)\n",
    "policy_random = np.array((np.random.permutation(2), np.random.permutation(2), np.random.permutation(2), np.random.permutation(2)))\n",
    "print(\"Research policy: \\n\",policy_R)\n",
    "print(\"Development policy: \\n\", policy_D)\n",
    "print(\"Random policy: \\n\",policy_random)\n",
    "\n",
    "policy_uncertain = np.concatenate((0.5*np.ones([4, 1]), 0.5*np.ones([4, 1])), axis=1)\n",
    "print(\"Uncertain policy: \\n\",policy_uncertain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00cfd0",
   "metadata": {
    "id": "ed00cfd0"
   },
   "source": [
    "### E1.3 Testing\n",
    "\n",
    "By usine one of the above policies, lets see how we navigate the environment. We want to see how we make take and action based on a given policy, what state we transition to and obtain the rewards from the transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3fd4869e",
   "metadata": {
    "id": "3fd4869e",
    "outputId": "e0dc70c0-be1d-469f-f1db-d1157bd19c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State\t Action\t New State\t Reward\t is_Terminal\n",
      "  0 \t   0 \t   2 \t 0.0 \t   False\n",
      "  2 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   0 \t   0 \t -10.0 \t   False\n",
      "  0 \t   0 \t   2 \t 0.0 \t   False\n",
      "  2 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   0 \t   0 \t -10.0 \t   False\n",
      "  0 \t   0 \t   2 \t 0.0 \t   False\n",
      "Total Number of steps: 10\n",
      "Final Reward: 480.0\n"
     ]
    }
   ],
   "source": [
    "env = CareerPathEnv()\n",
    "is_Terminal = False\n",
    "start_state = env.reset()\n",
    "steps = 0\n",
    "total_reward = 0\n",
    "\n",
    "# you may change policy here\n",
    "policy = policy_uncertain\n",
    "# policy = policy_R\n",
    "# policy = policy_D\n",
    "# policy = policy_random\n",
    "\n",
    "print(\"State\\t\", \"Action\\t\" , \"New State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "steps = 0\n",
    "max_steps = 5\n",
    "\n",
    "prev_state = start_state\n",
    "\n",
    "while steps < 10:\n",
    "    steps += 1\n",
    "\n",
    "    action = np.random.choice(2,1,p=policy[prev_state])[0]  #0 -> Research, 1 -> Development\n",
    "    state, reward, is_Terminal = env.step(action)\n",
    "\n",
    "    total_reward += reward\n",
    "\n",
    "    print(\" \",prev_state, \"\\t  \", action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    prev_state = state\n",
    "\n",
    "print(\"Total Number of steps:\", steps)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121e977-35c4-4845-888d-2b662262347a",
   "metadata": {
    "id": "9121e977-35c4-4845-888d-2b662262347a"
   },
   "source": [
    "### Iterative Policy Evaluation\n",
    "Iterative Policy Evaluation is commonly used to calculate the state value function $V_\\pi(s)$ for a given policy $\\pi$. Here we implement a function to compute the state value function $V_\\pi(s)$ for a given policy\n",
    "\n",
    "<img src='assets/policy_eval.png' width=\"500\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c360b73b-e4d4-4538-b654-5837a123ee11",
   "metadata": {
    "id": "c360b73b-e4d4-4538-b654-5837a123ee11"
   },
   "outputs": [],
   "source": [
    "# Policy Evaluation\n",
    "def EvaluatePolicy(env, policy, gamma=0.9, theta=1e-8, draw=False):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            Vs = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, done in env.P()[s][a]:\n",
    "                    Vs += action_prob * prob * (reward + gamma * V[next_state])\n",
    "            delta = max(delta, np.abs(V[s]-Vs))\n",
    "            V[s] = Vs\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b891c3a",
   "metadata": {
    "id": "7b891c3a"
   },
   "source": [
    "### Policy improvement\n",
    "\n",
    "$\\pi'(s) = \\arg \\max_a \\sum_{s',r} p(s',r|s,a)\\left[ r + \\gamma v_\\pi(s') \\right ]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "930db58c",
   "metadata": {
    "id": "930db58c"
   },
   "outputs": [],
   "source": [
    "##Policy Improvement Function\n",
    "def ImprovePolicy(env, v, gamma):\n",
    "    num_states = env.nS\n",
    "    num_actions = env.nA\n",
    "    prob_dynamics = env.P()\n",
    "\n",
    "    q = np.zeros((num_states, num_actions))\n",
    "\n",
    "    for state in prob_dynamics:\n",
    "            for action in prob_dynamics[state]:\n",
    "                #print(state, action)\n",
    "                for prob, new_state, reward, is_terminal in prob_dynamics[state][action]:\n",
    "                    #print(prob, new_state, reward, is_terminal)\n",
    "                    q[state][action] += prob*(reward + gamma*v[new_state])\n",
    "\n",
    "    new_pi = np.zeros((num_states, num_actions))\n",
    "\n",
    "    for state in range(num_states):\n",
    "        opt_action = np.argmax(q[state])\n",
    "        new_pi[state][opt_action] = 1.0\n",
    "\n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8634e11",
   "metadata": {
    "id": "d8634e11"
   },
   "source": [
    "### Policy Iteration\n",
    "\n",
    "<img src='assets/policy_iteration.png' width=\"500\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1860233b",
   "metadata": {
    "id": "1860233b"
   },
   "outputs": [],
   "source": [
    "def PolicyIteration(env, pi, gamma, tol = 1e-10):\n",
    "    num_states = env.nS\n",
    "    num_actions = env.nA\n",
    "    iterations = 0\n",
    "\n",
    "    while True:\n",
    "        # print(pi)\n",
    "        iterations += 1\n",
    "        pi_old = pi\n",
    "        v = EvaluatePolicy(env, pi_old, gamma, tol)\n",
    "        pi = ImprovePolicy(env, v, gamma)\n",
    "\n",
    "        is_equal = True\n",
    "        for s in range(num_states):\n",
    "            if np.argmax(pi_old[s]) == np.argmax(pi[s]):\n",
    "                continue\n",
    "            is_equal = False\n",
    "        if is_equal == True:\n",
    "            break\n",
    "    return pi, v, iterations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049ce61",
   "metadata": {
    "id": "5049ce61"
   },
   "source": [
    "### Testing Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9da9d38f",
   "metadata": {
    "id": "9da9d38f",
    "outputId": "dfcaf066-c998-4339-c29c-24633da4e496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Policy: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]]\n",
      "Final Policy: \n",
      " [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "State Value Function:  [1000. 1000. 1000. 1000.]\n",
      "Number of iterations for Policy Iteration:  2\n",
      "Iterations:\n",
      "Min\t Max\t Average\n",
      "1 \t 2 \t 1.94\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "env = CareerPathEnv()\n",
    "\n",
    "print(\"Initial Policy: \\n\",policy_random)\n",
    "pi, v, iters = PolicyIteration(env, policy_random, gamma)\n",
    "print(\"Final Policy: \\n\",pi)\n",
    "print(\"State Value Function: \",v)\n",
    "print(\"Number of iterations for Policy Iteration: \",iters)\n",
    "\n",
    "# average number of iterations required\n",
    "avg_iters = 0\n",
    "min_iters = 1000\n",
    "max_iters = 0\n",
    "for _ in range(100):\n",
    "    policy_random = np.array((np.random.permutation(2), np.random.permutation(2), np.random.permutation(2), np.random.permutation(2)))\n",
    "    _, _, iters = PolicyIteration(env,policy_random, gamma)\n",
    "    avg_iters += iters\n",
    "    min_iters = min(min_iters, iters)\n",
    "    max_iters = max(max_iters, iters)\n",
    "avg_iters /= 100\n",
    "print(\"Iterations:\")\n",
    "print(\"Min\\t\", \"Max\\t\" , \"Average\")\n",
    "print(min_iters,\"\\t\", max_iters,\"\\t\", avg_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f1d6c-dd20-4902-9581-ce7f8a0ec944",
   "metadata": {
    "id": "b57f1d6c-dd20-4902-9581-ce7f8a0ec944"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c086d4b9",
   "metadata": {
    "id": "c086d4b9"
   },
   "source": [
    "### A1. Find an optimal policy to navigate the given environment using Value Iteration (VI)\n",
    "\n",
    "<img src='assets/value_iteration.png' width=\"500\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "82e66db5",
   "metadata": {
    "id": "82e66db5"
   },
   "outputs": [],
   "source": [
    "# Value Iteration\n",
    "def ValueIteration(env, gamma=0.9, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Perform Value Iteration to find the optimal policy and value function for a given environment.\n",
    "\n",
    "    Args:\n",
    "        env: The environment to solve. It should have the following attributes:\n",
    "            - nS: Number of states.\n",
    "            - nA: Number of actions.\n",
    "            - prob_dynamics: A dictionary representing the transition probabilities, rewards, and terminal states.\n",
    "        gamma: Discount factor for future rewards. Default is 0.9.\n",
    "        max_iterations: Maximum number of iterations to perform. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        policy: A numpy array of shape (nS,) representing the optimal action to take in each state.\n",
    "        V: A numpy array of shape (nS,) representing the value function for each state.\n",
    "        i + 1: The number of iterations performed.\n",
    "\n",
    "    Example:\n",
    "        env = CareerPathEnv()\n",
    "        optimal_policy, optimal_value, iterations = ValueIteration(env)\n",
    "        print(\"Optimal Policy:\", optimal_policy)\n",
    "        print(\"Optimal Value Function:\", optimal_value)\n",
    "        print(\"Iterations:\", iterations)\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.nS)\n",
    "    for i in range(max_iterations):\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            A = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                for prob, next_state, reward, done in env.prob_dynamics[s][a]:\n",
    "                    A[a] += prob * (reward + gamma * V[next_state])\n",
    "            best_action_value = np.max(A)\n",
    "            delta = max(delta, abs(best_action_value - V[s]))\n",
    "            V[s] = best_action_value\n",
    "        if delta < 1e-10:\n",
    "            break\n",
    "\n",
    "    policy = np.zeros(env.nS, dtype=int)\n",
    "    for s in range(env.nS):\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.prob_dynamics[s][a]:\n",
    "                A[a] += prob * (reward + gamma * V[next_state])\n",
    "        policy[s] = np.argmax(A)\n",
    "\n",
    "    return policy, V, i + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d88b5",
   "metadata": {
    "id": "876d88b5"
   },
   "source": [
    "### Testing Value Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90c71594",
   "metadata": {
    "id": "90c71594"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      " [1 1 1 1]\n",
      "Optimal Value Function:\n",
      " [1000. 1000. 1000. 1000.]\n"
     ]
    }
   ],
   "source": [
    "# Test Value Iteration\n",
    "env = CareerPathEnv()\n",
    "optimal_policy, optimal_value,iters = ValueIteration(env)\n",
    "\n",
    "print(\"Optimal Policy:\\n\", optimal_policy)\n",
    "print(\"Optimal Value Function:\\n\", optimal_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf1175",
   "metadata": {
    "id": "83bf1175"
   },
   "source": [
    "### A1.2 Compare PI and VI in terms of convergence (average number of iteration, time required for each iteration). Is the policy obtained by both same?\n",
    "\n",
    "Write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "72658b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policies obtained by PI and VI are the same: False\n",
      "Number of iterations for Policy Iteration: 2\n",
      "Number of iterations for Value Iteration: 264\n"
     ]
    }
   ],
   "source": [
    "# PolicyIteration(env, policy, gamma)\n",
    "# ValueIteration(env, gamma)\n",
    "\n",
    "gamma = 0.9\n",
    "env = CareerPathEnv()\n",
    "\n",
    "# Perform Policy Iteration\n",
    "policy_random = np.array((np.random.permutation(2), np.random.permutation(2), np.random.permutation(2), np.random.permutation(2)))\n",
    "optimal_policy_pi, value_function_pi, pi_iters = PolicyIteration(env, policy_random, gamma)\n",
    "\n",
    "# Perform Value Iteration\n",
    "optimal_policy_vi, value_function_vi, vi_iters = ValueIteration(env, gamma)\n",
    "\n",
    "# Compare the policies\n",
    "policies_are_same = np.array_equal(optimal_policy_pi, optimal_policy_vi)\n",
    "\n",
    "print(\"Policies obtained by PI and VI are the same:\", policies_are_same)\n",
    "print(\"Number of iterations for Policy Iteration:\", pi_iters)\n",
    "print(\"Number of iterations for Value Iteration:\", vi_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc712ac",
   "metadata": {
    "id": "4cc712ac"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f6340",
   "metadata": {
    "id": "a47f6340"
   },
   "source": [
    "## Part B : A Stochastic Career Path\n",
    "\n",
    "Now consider a more realistic Markov Decision Process below with four states and two actions available at each state. In this setting Actions have nondeterministic effects, i.e., taking an action in a state always leads to one next state, but which state is the one next state is determined by transition probabilities. These transition probabilites are shown in the figure attached to the transition arrows from states and actions to states. There are two actions out of each state for the agent to choose from: D for development and R for research. The same _ultimately-care-only-about-money_ reward scheme is given along with the states.\n",
    "\n",
    "<img src='assets/mdp-nd.png' width=\"700\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "73a0ac3e-223e-42c9-896f-d3d74c060258",
   "metadata": {
    "id": "73a0ac3e-223e-42c9-896f-d3d74c060258"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Represents a Career Path problem Gym Environment which provides a Fully observable\n",
    "MDP\n",
    "'''\n",
    "class StochasticCareerPathEnv(gym.Env):\n",
    "    '''\n",
    "    StocasticCareerPathEnv represents the Gym Environment for the Career Path problem environment\n",
    "    States : [0:'Unemployed',1:'Industry',2:'Grad School',3:'Academia']\n",
    "    Actions : [0:'Research', 1:'Development']\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self,initial_state=3,no_states=4,no_actions=2):\n",
    "        '''\n",
    "        Constructor for the CareerPath class\n",
    "\n",
    "        Args:\n",
    "            initial_state : starting state of the agent\n",
    "            no_states : The no. of possible states which is 4\n",
    "            no_actions : The no. of possible actions which is 2\n",
    "\n",
    "        '''\n",
    "        self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "        self.nA = no_actions\n",
    "        self.nS = no_states\n",
    "        self.prob_dynamics = {\n",
    "            # s: {\n",
    "            #   a: [(p(s,s'|a), s', r', terminal/not), (p(s,s''|a), s'', r'', terminal/not)]\n",
    "            # }\n",
    "\n",
    "            0: {\n",
    "                0: [(1.0, 2, 0.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.9, 0, -10.0, False),(0.1, 1, 100, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.9, 3, 10.0, False),(0.1, 2, 0, False)],\n",
    "                1: [(0.9, 1, 100.0, False),(0.1, 1, 100, False)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(0.9, 1, 100.0, False),(0.1, 3, 10, False)],\n",
    "            },\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Resets the environment\n",
    "        Returns:\n",
    "            observations containing player's current state\n",
    "        '''\n",
    "        self.state = self.initial_state\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        '''\n",
    "        Returns the player's state as the observation of the environment\n",
    "        '''\n",
    "        return (self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        '''\n",
    "        Renders the environment\n",
    "        '''\n",
    "        print(\"Current state: {}\".format(self.state))\n",
    "\n",
    "    def sample_action(self):\n",
    "        '''\n",
    "        Samples and returns a random action from the action space\n",
    "        '''\n",
    "        return random.randint(0, self.nA)\n",
    "    def P(self):\n",
    "        '''\n",
    "        Defines and returns the probabilty transition matrix which is in the form of a nested dictionary\n",
    "        '''\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(1.0, 2, 0.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.9, 0, -10.0, False),(0.1, 1, 100, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.9, 3, 10.0, False),(0.1, 2, 0, False)],\n",
    "                1: [(0.9, 1, 100.0, False),(0.1, 1, 100, False)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(0.9, 1, 100.0, False),(0.1, 3, 10, False)],\n",
    "            },\n",
    "        }\n",
    "        return self.prob_dynamics\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Performs the given action\n",
    "        Args:\n",
    "            action : action from the action_space to be taking in the environment\n",
    "        Returns:\n",
    "            observation - returns current state\n",
    "            reward - reward obtained after taking the given action\n",
    "            done - True if the episode is complete else False\n",
    "        '''\n",
    "        if action >= self.nA:\n",
    "            action = self.nA-1\n",
    "\n",
    "        if self.state == 0 or (self.state == 1 and action == 1) or (self.state == 3 and action == 0):\n",
    "            index = 0\n",
    "        else:\n",
    "            index = np.random.choice(2,1,p=[0.9,0.1])[0]\n",
    "\n",
    "        dynamics_tuple = self.prob_dynamics[self.state][action][index]\n",
    "        self.state = dynamics_tuple[1]\n",
    "\n",
    "\n",
    "        return self.state, dynamics_tuple[2], dynamics_tuple[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a5212",
   "metadata": {
    "id": "4e6a5212"
   },
   "source": [
    "### Navigating in Stochastic Career Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d3e69064",
   "metadata": {
    "id": "d3e69064",
    "outputId": "8bba3991-3a27-43c3-dcca-5919f2277c1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State\t Action\t New State\t Reward\t is_Terminal\n",
      "  3 \t   0 \t   3 \t 10.0 \t   False\n",
      "  3 \t   0 \t   3 \t 10.0 \t   False\n",
      "  3 \t   0 \t   3 \t 10.0 \t   False\n",
      "  3 \t   0 \t   3 \t 10.0 \t   False\n",
      "  3 \t   0 \t   3 \t 10.0 \t   False\n",
      "  3 \t   0 \t   3 \t 10.0 \t   False\n",
      "  3 \t   0 \t   3 \t 10.0 \t   False\n",
      "  3 \t   0 \t   3 \t 10.0 \t   False\n",
      "  3 \t   0 \t   3 \t 10.0 \t   False\n",
      "  3 \t   0 \t   3 \t 10.0 \t   False\n",
      "Total Number of steps: 10\n",
      "Final Reward: 100.0\n"
     ]
    }
   ],
   "source": [
    "env = StochasticCareerPathEnv()\n",
    "is_Terminal = False\n",
    "start_state = env.reset()\n",
    "steps = 0\n",
    "total_reward = 0\n",
    "\n",
    "# you may change policy here\n",
    "policy = policy_random\n",
    "# policy = policy_1\n",
    "# policy = policy_2\n",
    "\n",
    "print(\"State\\t\", \"Action\\t\" , \"New State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "steps = 0\n",
    "max_steps = 5\n",
    "\n",
    "prev_state = start_state\n",
    "\n",
    "while steps < 10:\n",
    "    steps += 1\n",
    "\n",
    "    action = np.random.choice(2,1,p=policy[prev_state])[0]  #0 -> Research, 1 -> Development\n",
    "    state, reward, is_Terminal = env.step(action)\n",
    "\n",
    "    total_reward += reward\n",
    "\n",
    "    print(\" \",prev_state, \"\\t  \", action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    prev_state = state\n",
    "\n",
    "print(\"Total Number of steps:\", steps)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a3612",
   "metadata": {
    "id": "c01a3612"
   },
   "source": [
    "### B1.1 Find an optimal policy to navigate the given SCP environment using Policy Iteration (PI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eb1faa8a",
   "metadata": {
    "id": "eb1faa8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy using Policy Iteration:\n",
      " [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Optimal Value Function using Policy Iteration:\n",
      " [999.99999991 999.99999991 999.99999992 990.10989003]\n"
     ]
    }
   ],
   "source": [
    "# [Hint] What would change for the stochastic MDP in the Policy Iteration code from Part A?\n",
    "# write your code here\n",
    "\n",
    "# Policy Iteration for Stochastic MDP\n",
    "def PolicyIteration(env, gamma=0.9, theta=1e-8):\n",
    "    \"\"\"\n",
    "    Perform Policy Iteration to find the optimal policy and value function for a given environment.\n",
    "\n",
    "    Args:\n",
    "        env: The environment to solve. It should have the following attributes:\n",
    "            - nS: Number of states.\n",
    "            - nA: Number of actions.\n",
    "            - prob_dynamics: A dictionary representing the transition probabilities, rewards, and terminal states.\n",
    "        gamma: Discount factor for future rewards. Default is 0.9.\n",
    "        theta: A small threshold for determining the accuracy of estimation. Default is 1e-8.\n",
    "\n",
    "    Returns:\n",
    "        policy: A numpy array of shape (nS, nA) representing the optimal action to take in each state.\n",
    "        V: A numpy array of shape (nS,) representing the value function for each state.\n",
    "\n",
    "    Example:\n",
    "        env = StochasticCareerPathEnv()\n",
    "        optimal_policy, optimal_value = PolicyIteration(env)\n",
    "        print(\"Optimal Policy:\", optimal_policy)\n",
    "        print(\"Optimal Value Function:\", optimal_value)\n",
    "    \"\"\"\n",
    "    def EvaluatePolicy(policy):\n",
    "        \"\"\"\n",
    "        Evaluate a given policy to find the value function.\n",
    "\n",
    "        Args:\n",
    "            policy: A numpy array of shape (nS, nA) representing the policy to evaluate.\n",
    "\n",
    "        Returns:\n",
    "            V: A numpy array of shape (nS,) representing the value function for each state.\n",
    "        \"\"\"\n",
    "        V = np.zeros(env.nS)\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(env.nS):\n",
    "                v = V[s]\n",
    "                V[s] = sum([policy[s, a] * sum([prob * (reward + gamma * V[next_state])\n",
    "                                               for prob, next_state, reward, done in env.P()[s][a]])\n",
    "                            for a in range(env.nA)])\n",
    "                delta = max(delta, np.abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "    def ImprovePolicy(V):\n",
    "        \"\"\"\n",
    "        Improve the policy based on the current value function.\n",
    "\n",
    "        Args:\n",
    "            V: A numpy array of shape (nS,) representing the value function for each state.\n",
    "\n",
    "        Returns:\n",
    "            policy: A numpy array of shape (nS, nA) representing the improved policy.\n",
    "        \"\"\"\n",
    "        policy = np.zeros([env.nS, env.nA])\n",
    "        for s in range(env.nS):\n",
    "            Q = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                for prob, next_state, reward, done in env.P()[s][a]:\n",
    "                    Q[a] += prob * (reward + gamma * V[next_state])\n",
    "            best_action = np.argmax(Q)\n",
    "            policy[s, best_action] = 1.0\n",
    "        return policy\n",
    "\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    while True:\n",
    "        V = EvaluatePolicy(policy)\n",
    "        new_policy = ImprovePolicy(V)\n",
    "        if np.array_equal(new_policy, policy):\n",
    "            break\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "# Test Policy Iteration for Stochastic MDP\n",
    "env = StochasticCareerPathEnv()\n",
    "pi_policy, pi_value = PolicyIteration(env)\n",
    "\n",
    "print(\"Optimal Policy using Policy Iteration:\\n\", pi_policy)\n",
    "print(\"Optimal Value Function using Policy Iteration:\\n\", pi_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3264b19c",
   "metadata": {
    "id": "3264b19c"
   },
   "source": [
    "### B1.2 Find an optimal policy to navigate the given SCP environment using Value Iteration (VI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "77079cf6",
   "metadata": {
    "id": "77079cf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy using Value Iteration:\n",
      " [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Optimal Value Function using Value Iteration:\n",
      " [999.99999991 999.99999991 999.99999992 990.10989003]\n"
     ]
    }
   ],
   "source": [
    "# [Hint] What would change for the stochastic MDP in the Value Iteration code from Part A?\n",
    "# write your code here\n",
    "# Value Iteration for Stochastic MDP\n",
    "def ValueIteration(env, gamma=0.9, theta=1e-8):\n",
    "    \"\"\"\n",
    "    Perform Value Iteration to find the optimal policy and value function for a given environment.\n",
    "\n",
    "    Args:\n",
    "        env: The environment to solve. It should have the following attributes:\n",
    "            - nS: Number of states.\n",
    "            - nA: Number of actions.\n",
    "            - prob_dynamics: A dictionary representing the transition probabilities, rewards, and terminal states.\n",
    "        gamma: Discount factor for future rewards. Default is 0.9.\n",
    "        theta: A small threshold for determining the accuracy of estimation. Default is 1e-8.\n",
    "\n",
    "    Returns:\n",
    "        policy: A numpy array of shape (nS, nA) representing the optimal action to take in each state.\n",
    "        V: A numpy array of shape (nS,) representing the value function for each state.\n",
    "\n",
    "    Example:\n",
    "        env = StochasticCareerPathEnv()\n",
    "        optimal_policy, optimal_value = ValueIteration(env)\n",
    "        print(\"Optimal Policy:\", optimal_policy)\n",
    "        print(\"Optimal Value Function:\", optimal_value)\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            v = V[s]\n",
    "            Q = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                for prob, next_state, reward, done in env.P()[s][a]:\n",
    "                    Q[a] += prob * (reward + gamma * V[next_state])\n",
    "            V[s] = np.max(Q)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    for s in range(env.nS):\n",
    "        Q = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P()[s][a]:\n",
    "                Q[a] += prob * (reward + gamma * V[next_state])\n",
    "        best_action = np.argmax(Q)\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, V\n",
    "\n",
    "# Test Value Iteration for Stochastic MDP\n",
    "env = StochasticCareerPathEnv()\n",
    "vi_policy, vi_value = ValueIteration(env)\n",
    "\n",
    "print(\"Optimal Policy using Value Iteration:\\n\", vi_policy)\n",
    "print(\"Optimal Value Function using Value Iteration:\\n\", vi_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda774d8",
   "metadata": {
    "id": "dda774d8"
   },
   "source": [
    "### B1.3  Compare PI and VI in terms of convergence (average number of iteration, time required for each iteration). Is the policy obtained by both same for SCP environment?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5c6f9de8",
   "metadata": {
    "id": "5c6f9de8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration Policy:\n",
      " [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Value Iteration Policy:\n",
      " [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Are policies the same? True\n",
      "Policy Iteration Value Function:\n",
      " [999.99999991 999.99999991 999.99999992 990.10989003]\n",
      "Value Iteration Value Function:\n",
      " [999.99999991 999.99999991 999.99999992 990.10989003]\n",
      "Are value functions the same? True\n"
     ]
    }
   ],
   "source": [
    "# write your code for comparison here\n",
    "# Compare Policies and Values\n",
    "print(\"Policy Iteration Policy:\\n\", pi_policy)\n",
    "print(\"Value Iteration Policy:\\n\", vi_policy)\n",
    "print(\"Are policies the same?\", np.array_equal(pi_policy, vi_policy))\n",
    "\n",
    "print(\"Policy Iteration Value Function:\\n\", pi_value)\n",
    "print(\"Value Iteration Value Function:\\n\", vi_value)\n",
    "print(\"Are value functions the same?\", np.allclose(pi_value, vi_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61e7a8",
   "metadata": {
    "id": "5a61e7a8"
   },
   "source": [
    "Write your comments compairing convergence and policies here."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ML_BASE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
